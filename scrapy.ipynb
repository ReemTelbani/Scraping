{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890397fc-6f22-4bf3-bd92-37e9201f1f77",
   "metadata": {},
   "source": [
    "## Task Overview:\n",
    ">The task would be Developing a service that \n",
    "takes a request on which it will scrap Ads data from OLX. \n",
    "\n",
    ">Where the service has an API that receives a request containing query information, \n",
    "process this request, \n",
    "starts scraping Ads data according to the query, \n",
    "stores all collected results in database and forwards a sample of the results to an email. \n",
    "\n",
    ">input query --> process --> scrap  --> store  --> send mail\n",
    "\n",
    "## Requirements \n",
    ">• Build an API that receives a request containing\n",
    ">>a search keyword (The search keyword atribute would be the text value that you'll search for on OLX platorm and then scrap the Ads results based on it, and the collected results must be 300 ad rows at least)\n",
    ">>\n",
    ">>an email (While the email atribute represents the email to which you'll send a sample of the collected Ads results. )\n",
    ">>\n",
    ">>and a size atribute (The size atribute would be the size of the results sample sent to the specified email with minimum value of 20 results. )\n",
    "> \n",
    ">• A scrapped Ad should contain all Ad's related product/service details including\n",
    ">>-price DONE\n",
    ">>\n",
    ">>-the advertiser's name\n",
    ">>\n",
    ">>-phone\n",
    ">>\n",
    ">>-locaton DONE\n",
    ">>\n",
    ">>-any additonal details\n",
    ">\n",
    ">• The collected data results will be stored in mongodb.\n",
    ">\n",
    ">•A sample of the results will be sent to the specified email in the API request,\n",
    ">>\n",
    ">>-and the sample size would be already set earlier in the API request.\n",
    ">>\n",
    ">>• Before sending the sample Ads results to the email, these results should be sorted according to price. \n",
    ">\n",
    ">>• If a query is repeated on the same day, The results should be returned from the mongodb and not scrapped from OLX, as the requested data would be already stored in the database. \n",
    ">\n",
    ">• The task should be done in python. \n",
    ">\n",
    ">• Selenium and Flask should be used in this task among the used tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572d049-7a32-48d9-8ee8-40ef8934d18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>> APIs are up running...\n",
      ">>>>>>>>>>> Application Works..\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:4996\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Request 'http://127.0.0.1:4996/retrive_ads' [POST]>\n",
      "request printed\n",
      "1-PROCESS_REQUEST Status Success...\n",
      "دوبيزل (أوليكس) - بيع واشتري من أي مكان في مصر على موقع دوبيزل (أوليكس) للإعلانات المبوبة\n",
      "1-search_with_keyword.... \n",
      "searching with: لابتوب\n",
      "1-search_with_keyword done....\n",
      "\n",
      "2-scarping_ads_count... \n",
      "Ads Count: 6040\n",
      "2-scarping_ads_count done...\n",
      "\n",
      "requested ads is suitable...\n",
      "********************************************\n",
      "requested_returned_ads_count:  300\n",
      "ads_count:  6040\n",
      "target_number_of_ads:  300\n",
      "********************************************\n",
      "3- scarping_ads_links start TODO\n",
      "len(links)  45\n",
      "Number of returned ads from page number:  1  is  45\n",
      "3- scarping_ads_links done TODO\n",
      "ChromeDriver is initialized and ready to use.\n",
      "retrived_items_counter:  45\n",
      "retrive from other pages.\n",
      "scrapy didnt reach to the target_number_of_ads, loop till reach the target number...\n",
      " 4- navigate_to_the_next_page start\n",
      "navigate to page number:----> 2\n",
      "Extracted href: https://www.dubizzle.com.eg/ads/q-%D9%84%D8%A7%D8%A8%D8%AA%D9%88%D8%A8/?page=2\n",
      "href_value_new : https://www.dubizzle.com.eg/ads/q-%D9%84%D8%A7%D8%A8%D8%AA%D9%88%D8%A8/?=page=2\n",
      "لابتوب في مصر ، إعلانات مبوبة في مصر | دوبيزل مصر (أوليكس)\n",
      "4- navigate_to_the_next_page done \n",
      "\n",
      "3- scarping_ads_links start TODO\n",
      "len(links)  45\n",
      "Number of returned ads from page number:  2  is  45\n",
      "3- scarping_ads_links done TODO\n",
      "ChromeDriver is initialized and ready to use.\n",
      "retrived_items_counter:  90\n",
      "crapping other pages in progress\n",
      "scrapy didnt reach to the target_number_of_ads, loop till reach the target number...\n",
      " 4- navigate_to_the_next_page start\n",
      "navigate to page number:----> 3\n",
      "Extracted href: https://www.dubizzle.com.eg/ads/q-%D9%84%D8%A7%D8%A8%D8%AA%D9%88%D8%A8/?page=3\n",
      "href_value_new : https://www.dubizzle.com.eg/ads/q-%D9%84%D8%A7%D8%A8%D8%AA%D9%88%D8%A8/?=page=3\n",
      "لابتوب في مصر ، إعلانات مبوبة في مصر | دوبيزل مصر (أوليكس)\n",
      "4- navigate_to_the_next_page done \n",
      "\n",
      "3- scarping_ads_links start TODO\n",
      "len(links)  45\n",
      "Number of returned ads from page number:  3  is  45\n",
      "3- scarping_ads_links done TODO\n",
      "ChromeDriver is initialized and ready to use.\n",
      "retrived_items_counter:  135\n",
      "crapping other pages in progress\n",
      "scrapy didnt reach to the target_number_of_ads, loop till reach the target number...\n",
      " 4- navigate_to_the_next_page start\n",
      "navigate to page number:----> 4\n",
      "Extracted href: https://www.dubizzle.com.eg/ads/q-%D9%84%D8%A7%D8%A8%D8%AA%D9%88%D8%A8/?page=4\n",
      "href_value_new : https://www.dubizzle.com.eg/ads/q-%D9%84%D8%A7%D8%A8%D8%AA%D9%88%D8%A8/?=page=4\n",
      "لابتوب في مصر ، إعلانات مبوبة في مصر | دوبيزل مصر (أوليكس)\n",
      "4- navigate_to_the_next_page done \n",
      "\n",
      "3- scarping_ads_links start TODO\n",
      "len(links)  45\n",
      "Number of returned ads from page number:  4  is  45\n",
      "3- scarping_ads_links done TODO\n",
      "ChromeDriver is initialized and ready to use.\n",
      "retrived_items_counter:  180\n",
      "crapping other pages in progress\n",
      "scrapy didnt reach to the target_number_of_ads, loop till reach the target number...\n",
      " 4- navigate_to_the_next_page start\n",
      "navigate to page number:----> 5\n",
      "Extracted href: https://www.dubizzle.com.eg/ads/q-%D9%84%D8%A7%D8%A8%D8%AA%D9%88%D8%A8/?page=5\n",
      "href_value_new : https://www.dubizzle.com.eg/ads/q-%D9%84%D8%A7%D8%A8%D8%AA%D9%88%D8%A8/?=page=5\n",
      "لابتوب في مصر ، إعلانات مبوبة في مصر | دوبيزل مصر (أوليكس)\n",
      "4- navigate_to_the_next_page done \n",
      "\n",
      "3- scarping_ads_links start TODO\n",
      "len(links)  45\n",
      "Number of returned ads from page number:  5  is  45\n",
      "3- scarping_ads_links done TODO\n",
      "ChromeDriver is initialized and ready to use.\n",
      "retrived_items_counter:  225\n",
      "crapping other pages in progress\n",
      "scrapy didnt reach to the target_number_of_ads, loop till reach the target number...\n",
      " 4- navigate_to_the_next_page start\n",
      "navigate to page number:----> 6\n",
      "Extracted href: https://www.dubizzle.com.eg/ads/q-%D9%84%D8%A7%D8%A8%D8%AA%D9%88%D8%A8/?page=6\n",
      "href_value_new : https://www.dubizzle.com.eg/ads/q-%D9%84%D8%A7%D8%A8%D8%AA%D9%88%D8%A8/?=page=6\n",
      "لابتوب في مصر ، إعلانات مبوبة في مصر | دوبيزل مصر (أوليكس)\n",
      "4- navigate_to_the_next_page done \n",
      "\n",
      "3- scarping_ads_links start TODO\n",
      "len(links)  45\n",
      "Number of returned ads from page number:  6  is  45\n",
      "3- scarping_ads_links done TODO\n",
      "ChromeDriver is initialized and ready to use.\n",
      "retrived_items_counter:  270\n",
      "crapping other pages in progress\n",
      "scrapy didnt reach to the target_number_of_ads, loop till reach the target number...\n",
      " 4- navigate_to_the_next_page start\n",
      "navigate to page number:----> 7\n",
      "Extracted href: https://www.dubizzle.com.eg/ads/q-%D9%84%D8%A7%D8%A8%D8%AA%D9%88%D8%A8/?page=7\n",
      "href_value_new : https://www.dubizzle.com.eg/ads/q-%D9%84%D8%A7%D8%A8%D8%AA%D9%88%D8%A8/?=page=7\n",
      "لابتوب في مصر ، إعلانات مبوبة في مصر | دوبيزل مصر (أوليكس)\n",
      "4- navigate_to_the_next_page done \n",
      "\n",
      "3- scarping_ads_links start TODO\n",
      "len(links)  45\n",
      "Number of returned ads from page number:  7  is  45\n",
      "3- scarping_ads_links done TODO\n",
      "ChromeDriver is initialized and ready to use.\n",
      "retrived_items_counter:  315\n",
      "crapping other pages in progress\n",
      "scrapy reached to the target_number_of_ads, remove extra retrived items...\n",
      "type all_ads_fetched:  <class 'list'>\n",
      "all_ads_fetched:  7\n",
      "all_ads_fetched:  315\n",
      "all_ads_fetched:  301\n",
      "print i quit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [13/Jan/2025 09:25:51] \"POST /retrive_ads HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-RETRIVE_DATA Status Success...\n"
     ]
    }
   ],
   "source": [
    "'''======== IMPORTING PACKAGES =============================================================''' \n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from dotenv import load_dotenv\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_mail import Mail, Message\n",
    "import smtplib\n",
    "import json\n",
    "import re\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "#imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "import requests, uuid, json\n",
    "from flask import Flask , jsonify, request\n",
    "import time \n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "\n",
    "'''======== IMPORTING PACKAGES =============================================================''' \n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from dotenv import load_dotenv\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_mail import Mail, Message\n",
    "import smtplib\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env_file = \".env\"\n",
    "load_dotenv(dotenv_path=env_file)\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "def random_delay(min_sec=1, max_sec=5):\n",
    "    time.sleep(random.uniform(min_sec, max_sec))\n",
    "\n",
    "def simulate_typing(element, text):\n",
    "    for char in text:\n",
    "        element.send_keys(char)\n",
    "        random_delay(0.1, 0.3)  # Simulate typing speed\n",
    "\n",
    "\n",
    "\n",
    "def smooth_scroll(driver, min_h=500, max_h=800):\n",
    "    height = random.uniform(min_h, max_h)\n",
    "    \"\"\"Smoothly scroll down the page by the specified height.\"\"\"\n",
    "    current_height = driver.execute_script(\"return window.pageYOffset;\")\n",
    "    target_height = current_height + height\n",
    "    while current_height < target_height:\n",
    "        driver.execute_script(\"window.scrollTo(0, arguments[0]);\", current_height)\n",
    "        current_height += 1  # Adjust this value for faster/slower scrolling\n",
    "        time.sleep(0.01)  # Adjust delay for smoother scrolling\n",
    "        \n",
    "def mimic_human(driver):\n",
    "    smooth_scroll(driver, 500,800)\n",
    "    random_delay()\n",
    "\n",
    "\n",
    "def save_to_mongo(data, db_name, collection_name):\n",
    "    # Step 1: Create a MongoDB client\n",
    "    client = MongoClient('mongodb://localhost:27017/')  # Adjust the URI as needed\n",
    "\n",
    "    # Step 2: Access the database\n",
    "    db = client[db_name]\n",
    "\n",
    "    # Step 3: Access the collection\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    # Step 4: Insert the dictionary into the collection\n",
    "    result = collection.insert_one(data)\n",
    "\n",
    "    # Print the inserted ID\n",
    "    #print(f'Document inserted with ID: {result.inserted_id}')\n",
    "\n",
    "'''\n",
    "def retrieve_from_mongo(db_name, collection_name, query=None):\n",
    "    # Step 1: Create a MongoDB client\n",
    "    client = MongoClient('mongodb://localhost:27017/')  # Adjust as needed\n",
    "\n",
    "    # Step 2: Access the database\n",
    "    db = client[db_name]\n",
    "\n",
    "    # Step 3: Access the collection\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    # Step 4: Query the collection\n",
    "    if query is None:\n",
    "        # Retrieve all documents if no query is provided\n",
    "        results = collection.find()\n",
    "    else:\n",
    "        # Retrieve documents that match the query\n",
    "        results = collection.find(query)\n",
    "\n",
    "    # Step 5: Process and print the results\n",
    "    #for document in results:\n",
    "    #    print(document)\n",
    "'''\n",
    "\n",
    "def retrieve_from_mongo(db_name, collection_name, query):\n",
    "    # Step 1: Create a MongoDB client\n",
    "    client = MongoClient('mongodb://localhost:27017/')  # Adjust as needed\n",
    "\n",
    "    # Step 2: Access the database\n",
    "    db = client[db_name]\n",
    "\n",
    "    # Step 3: Access the collection\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    # Step 4: Query the collection\n",
    "    if query is None:\n",
    "        # Retrieve all documents if no query is provided\n",
    "        results = \"\"\n",
    "        status = False\n",
    "    else:\n",
    "        # Step 5: Retrieve and sort the documents by price in descending order\n",
    "        results = collection.find(query).sort(\"price\", -1)\n",
    "        status=True\n",
    "\n",
    "    # Step 6: Process and return the results\n",
    "    return status, list(results)\n",
    "\n",
    "\n",
    "\n",
    "def categorize_listing(text):\n",
    "    # Split the listing into lines\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Initialize variables\n",
    "    extra_info = None\n",
    "    price = None\n",
    "    description = None\n",
    "    ad_creation_time = None\n",
    "\n",
    "\n",
    "    # Check if the first line contains \"مميز\"\n",
    "    if \"مميز\" in lines[0]:\n",
    "        extra_info = lines[0].strip()  # First line as extra info\n",
    "\n",
    "    # Check for price in the relevant lines\n",
    "    if \"ج.م\" in lines[1]:\n",
    "        price = lines[1].strip()\n",
    "    elif \"ج.م\" in lines[0]:\n",
    "        price = lines[0].strip()\n",
    "    else:\n",
    "        price = None\n",
    "\n",
    "    if \"منذ\" in lines[2]:\n",
    "        temp_list= lines[2].strip().split('منذ')\n",
    "        ad_creation_time = temp_list[1]\n",
    "        place= temp_list[0]\n",
    "        \n",
    "    elif \"منذ\" in lines[3]:\n",
    "        temp_list = lines[3].strip().split('منذ')\n",
    "        ad_creation_time = temp_list[1]\n",
    "        place= temp_list[0]\n",
    "        \n",
    "    elif \"منذ\" in lines[4]:\n",
    "        temp_list = lines[4].strip().split('منذ')\n",
    "        ad_creation_time = temp_list[1]\n",
    "        place= temp_list[0]\n",
    "        \n",
    "    else:\n",
    "        ad_creation_time = None\n",
    "        place = None\n",
    "\n",
    "    # Combine remaining lines for description\n",
    "    description = \" \".join(lines[1:])  # Combine to form the description\n",
    "\n",
    "    # Create a dictionary for the current listing\n",
    "    categorized_listing = {\n",
    "        'price': price,\n",
    "        'ad_creation_time_since': ad_creation_time,\n",
    "        'description': description,\n",
    "        'place':place,\n",
    "        'special_ad': extra_info,\n",
    "        \n",
    "    }\n",
    "    return categorized_listing\n",
    "\n",
    "\n",
    "def extract_number(text):\n",
    "    # Use regular expressions to find all digits in the text\n",
    "    match = re.search(r'\\d+', text)\n",
    "    if match:\n",
    "        # Convert the found digits to an integer\n",
    "        return int(match.group(0))\n",
    "    else:\n",
    "        # Return None if no digits are found\n",
    "        return 0\n",
    "\n",
    "       \n",
    "\n",
    "def categorize_listing(text):\n",
    "    # Split the listing into lines\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Initialize variables\n",
    "    extra_info = None\n",
    "    price = None\n",
    "    description = None\n",
    "    ad_creation_time = None\n",
    "    place = None\n",
    "    \n",
    "    # Check for the word \"منذ\" in each line\n",
    "    for line in lines:\n",
    "        if \"منذ\" in line:\n",
    "            # Print the line for debugging\n",
    "            #print(f\"The word 'منذ' is in line: {line}\")\n",
    "            temp_list = line.strip().split('منذ')\n",
    "            if len(temp_list) > 1:\n",
    "                place = temp_list[0].strip()\n",
    "                ad_creation_time = temp_list[1].strip()\n",
    "        \n",
    "        if \"ج.م\" in line:\n",
    "            # Print the line for debugging\n",
    "            price = line.strip()\n",
    "\n",
    "        if \"مميز\" in line:\n",
    "            # Print the line for debugging\n",
    "            extra_info = line.strip()\n",
    "\n",
    "    # Combine remaining lines for description, except the first line\n",
    "    description = \" \".join(lines[1:])\n",
    "\n",
    "    # Create a dictionary for the current listing\n",
    "    categorized_listing = {\n",
    "        'price': price,\n",
    "        'ad_creation_time_since': ad_creation_time,\n",
    "        'description': description,\n",
    "        'place': place,\n",
    "        'special_ad': extra_info,\n",
    "    }\n",
    "    \n",
    "    return categorized_listing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def old_scrape_page(driver, ad_links, page_number, search_keyword):\n",
    "    links = []\n",
    "    for link in ad_links:\n",
    "        href = link.get_attribute('href')\n",
    "        if href not in links:\n",
    "            links.append(href)\n",
    "    #for i in links:\n",
    "    #    print(i)\n",
    "    print(\"len(links) \",len(links))\n",
    "    \n",
    "    # Find all elements that match the CSS selector \n",
    "    list_ele = driver.find_elements(By.CSS_SELECTOR, \"[aria-label='Listing']\") \n",
    "\n",
    "    # Loop over the elements and print their text \n",
    "    c=0\n",
    "    ads_list = []\n",
    "    for element in list_ele: \n",
    "        #print(element.text)\n",
    "        #print(\"\")\n",
    "        text = element.text\n",
    "        result = categorize_listing(text)\n",
    "        result['ad_link'] = links[c]\n",
    "        result['insertion_date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "        result['page_number']=page_number\n",
    "        result['search_keyword']=search_keyword\n",
    "        result['links_count_in_page']= len(ad_links)\n",
    "        ads_list.append(result)\n",
    "        #ads_dic[c]=text\n",
    "        # Save the data to MongoDB\n",
    "        save_to_mongo(result, 'olx_ads', 'ads_temp')\n",
    "        c=c+1\n",
    "    #print(\"ads_dic.keys \",len(ads_dic.keys()))\n",
    "    #print(ads_dic) \n",
    "    print(\"Number of returned ads from page number: \",str(page_number), \" is \",str(len(ads_list)))\n",
    "    \n",
    "    return True, ads_list\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def scrape_page_flow(driver, page_number, search_keyword):\n",
    "    # Check if the ChromeDriver is initialized\n",
    "    if driver is not None:\n",
    "        ad_links = driver.find_elements(By.CSS_SELECTOR, 'a[href^=\"/ad/\"]')\n",
    "        #print(f\"Number of matching links: {len(set(ad_links))}\")\n",
    "        mimic_human(driver)\n",
    "    \n",
    "        print(\"3- scarping_ads_links start TODO\")\n",
    "        page_scrapping_status, ads_list = old_scrape_page(driver, ad_links, page_number, search_keyword)\n",
    "        print(\"3- scarping_ads_links done TODO\")\n",
    "    \n",
    "        if page_scrapping_status:\n",
    "            counted_items= len(ads_list)\n",
    "        else:\n",
    "            counted_items=0\n",
    "        print(\"ChromeDriver is initialized and ready to use.\")\n",
    "    else:\n",
    "        page_scrapping_status=False\n",
    "        counted_items=0\n",
    "        print(\"ChromeDriver failed to initialize.\")\n",
    "        \n",
    "    return  page_scrapping_status, counted_items, ads_list\n",
    "\n",
    "\n",
    "def navigate_to_the_next_page(driver, page_number):\n",
    "        try:\n",
    "            print(\" 4- navigate_to_the_next_page start\")\n",
    "            print(\"navigate to page number:---->\", page_number)\n",
    "            \n",
    "        \n",
    "            # Locate the <div> with role=\"navigation\"\n",
    "            navigation_element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '//div[@role=\"navigation\"]'))\n",
    "            )\n",
    "        \n",
    "            mimic_human(driver)\n",
    "        \n",
    "            last_li = navigation_element.find_element(By.XPATH, './ul/li[last()]')\n",
    "            last_a = last_li.find_element(By.TAG_NAME, 'a')\n",
    "            href_value = last_a.get_attribute('href')\n",
    "            print(\"Extracted href:\", href_value)\n",
    "            \n",
    "            href_value_new = href_value.split('page=')[0]+\"=page=\"+str(page_number)\n",
    "            print(\"href_value_new :\", href_value_new)\n",
    "            \n",
    "            #href_value = \"/ads/q-\"+query+\"/?page=\"+str(page_number)\n",
    "            #next_point = \"https://www.dubizzle.com.eg/ads/q-\"+query+\"/?page=\"+str(page_number)\n",
    "            #print(next_point)\n",
    "            \n",
    "            # add the page count to the page number iteratively\n",
    "            #driver.get(next_point)\n",
    "            \n",
    "            \n",
    "    \n",
    "            #print(href_value)\n",
    "            #print(\"search for href\")\n",
    "    \n",
    "            driver.execute_script(\"window.open(arguments[0], '_self');\", href_value)\n",
    "            time.sleep(20)\n",
    "            print(driver.title)\n",
    "\n",
    "            if (driver.title!='خطأ داخلي | دوبيزل مصر (أوليكس)'):\n",
    "                print(\"4- navigate_to_the_next_page done \")\n",
    "                print(\"\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"4- navigate to the next page failed\")\n",
    "                print(\"\")\n",
    "                return False\n",
    "                \n",
    "\n",
    "        except:\n",
    "            return False\n",
    "            print(\"4- navigate to the next page failed\")\n",
    "            print(\"\")\n",
    "\n",
    "def main_page_flow(driver, query):\n",
    "    print(\"1-search_with_keyword.... \")\n",
    "    search = driver.find_element(By.XPATH, '//*[@id=\"body-wrapper\"]/div[1]/header/div[2]/div[2]/div/div/div/div/div[1]/input')\n",
    "    #search= driver.find_element(By.CSS_SELECTOR, \"input[type='search'][autocomplete='free-text-search']\")\n",
    "    mimic_human(driver)\n",
    "    \n",
    "    print(\"searching with:\", query)\n",
    "    simulate_typing(search, query)\n",
    "    \n",
    "    search.send_keys(Keys.RETURN)\n",
    "    print(\"1-search_with_keyword done....\")\n",
    "    print(\"\")\n",
    "    random_delay()\n",
    "\n",
    "    print(\"2-scarping_ads_count... \")\n",
    "    # Collect ads count\n",
    "    ads_count_element = driver.find_elements(By.XPATH, '//*[@id=\"body-wrapper\"]/div[2]/header[2]/div/div/div/div[2]/div[1]/div[2]/div/div')\n",
    "    if ads_count_element:\n",
    "        ads_count = int(re.sub(r'[^a-zA-Z0-9]', '', ads_count_element[0].text))\n",
    "    else:\n",
    "        ads_count=0\n",
    "    print(\"Ads Count:\", ads_count)\n",
    "    print(\"2-scarping_ads_count done...\")\n",
    "    print(\"\")\n",
    "    return driver, ads_count\n",
    "\n",
    "def SCRAPER(keyword, requested_returned_ads_count):\n",
    "       \n",
    "        # Set up Chrome options\n",
    "        options = Options()\n",
    "        #ua = UserAgent()\n",
    "        #options.add_argument(f'user-agent={ua.random}')\n",
    "        #options.add_argument(\"--incognito\")\n",
    "        #options.add_argument(\"--headless\")  # Run in headless mode\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Help prevent detection ,  adding argument to disable the AutomationControlled flag \n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"]) # exclude the collection of enable-automation switches \n",
    "        options.add_experimental_option(\"useAutomationExtension\", False)  # turn-off userAutomationExtension \n",
    "        options.add_argument(\"window-size=1920,1080\")\n",
    "        options.add_argument(\"--disable-extensions\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--ignore-certificate-errors\")\n",
    "    \n",
    "        # List of proxy servers\n",
    "        proxies = [\n",
    "            'http://184.82.55.109:8080',\n",
    "            'http://188.166.229.121:80',\n",
    "            'http://49.49.60.99:8080'\n",
    "        ]\n",
    "        # Function to get a random proxy\n",
    "        def get_random_proxy():\n",
    "            return random.choice(proxies)\n",
    "    \n",
    "        status = False\n",
    "        query = keyword\n",
    "        \n",
    "    # Loop through proxies\n",
    "   # for _ in range(5):  # Adjust the number of attempts as needed\n",
    "        \n",
    "        #proxy = get_random_proxy()\n",
    "        #options.add_argument(f'--proxy-server={proxy}')\n",
    "\n",
    "        end_point =\"https://www.dubizzle.com.eg/\"\n",
    "\n",
    "        # Initialize the Chrome driver\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\") # changing the property of the navigator value for webdriver to undefined\n",
    "        \n",
    "        driver.get(end_point)\n",
    "        print(driver.title)\n",
    "        \n",
    "        random_delay()\n",
    "        \n",
    "        \n",
    "    \n",
    "        try:\n",
    "            retrived_items_counter = 0\n",
    "            all_ads_fetched = []\n",
    "            driver, ads_count = main_page_flow(driver, query)\n",
    "    \n",
    "            target_number_of_ads =0\n",
    "    \n",
    "            # 2<150\n",
    "\n",
    "            \n",
    "            if (requested_returned_ads_count<=ads_count) :\n",
    "                print(\"requested ads is suitable...\")\n",
    "                target_number_of_ads= requested_returned_ads_count\n",
    "                retriving_count_message=\"we will retrive for you \"\n",
    "    \n",
    "            else:\n",
    "                print(\"requested ads is Not suitable...\")\n",
    "                #print(\"requested ads more than the existing ads in the website, retrive all the ads found as much as the scraper can get...\")\n",
    "                target_number_of_ads=ads_count\n",
    "                retriving_count_message= '''you requested ads more than the avilable ads exist on OLX, \n",
    "                                            try generalize your search keyword....'''\n",
    "\n",
    "            print(\"********************************************\")\n",
    "            print(\"requested_returned_ads_count: \",requested_returned_ads_count)\n",
    "            print(\"ads_count: \",ads_count)\n",
    "            print(\"target_number_of_ads: \",target_number_of_ads)\n",
    "            print(\"********************************************\")\n",
    "    \n",
    "            #scrape 1st page:\n",
    "            page_scrape_status, retrived_ads_count_from_page_1, ads_list = scrape_page_flow(driver, 1, keyword)\n",
    "            all_ads_fetched.append(ads_list)\n",
    "            \n",
    "            retrived_items_counter = retrived_items_counter + retrived_ads_count_from_page_1\n",
    "            print(\"retrived_items_counter: \", retrived_items_counter)\n",
    "            \n",
    "            if target_number_of_ads< retrived_ads_count_from_page_1: # no need to navigate to page 2, 3 , 4, scrape only page 1\n",
    "                print(\"retrive from page 1 only.\")\n",
    "                retriving_count_message = retriving_count_message + str(target_number_of_ads) +\" ads.\"\n",
    "                return True\n",
    "    \n",
    "            else:\n",
    "                print(\"retrive from other pages.\")\n",
    "                #print(\"the targeted number of ads is more than the retrived_ads_count from page 1, you have to navigate to page 2, 3, 4...\")\n",
    "                pages_counter=1\n",
    "                number_to_return = 0\n",
    "                \n",
    "                while True:\n",
    "                    pages_counter = pages_counter+1\n",
    "                    #80<100 , 110>100\n",
    "                    if (retrived_items_counter< target_number_of_ads):\n",
    "                        \n",
    "                        print(\"scrapy didnt reach to the target_number_of_ads, loop till reach the target number...\")\n",
    "                        navigate_to_the_next_page_status = navigate_to_the_next_page(driver , pages_counter)\n",
    "                        \n",
    "                        if navigate_to_the_next_page_status:\n",
    "                            time.sleep(20)\n",
    "                            page_scrape_status, retrived_ads_count, ads_list  = scrape_page_flow(driver, pages_counter, keyword)\n",
    "                            all_ads_fetched.append(ads_list)\n",
    "                            \n",
    "                            \n",
    "                            if page_scrape_status:\n",
    "                                retrived_items_counter = retrived_items_counter + retrived_ads_count\n",
    "                                print(\"retrived_items_counter: \", retrived_items_counter)\n",
    "                                number_to_return = retrived_items_counter\n",
    "                                print(\"crapping other pages in progress\")\n",
    "                                continue\n",
    "                                \n",
    "                            else:\n",
    "                                number_to_return = retrived_items_counter\n",
    "                                print(\"error could be happen while scrapping other pages\")\n",
    "                                break\n",
    "    \n",
    "                                \n",
    "                        else:\n",
    "                            number_to_return = retrived_items_counter\n",
    "                            print(\"error could be happen while navigating to other pages\")\n",
    "                            break\n",
    "\n",
    "                        print(\"number_to_return: \", number_to_return)\n",
    "                            \n",
    "                        \n",
    "                       # continue\n",
    "    \n",
    "                    else:\n",
    "                        print(\"scrapy reached to the target_number_of_ads, remove extra retrived items...\")\n",
    "                        number_to_return = target_number_of_ads\n",
    "                        #ads_list = ads_list[:target_number_of_ads])\n",
    "                        break\n",
    "\n",
    "                    \n",
    "            print(\"type all_ads_fetched: \", type(all_ads_fetched))\n",
    "            print(\"all_ads_fetched: \", len(all_ads_fetched))\n",
    "            all_ads_fetched = [item for sublist in all_ads_fetched for item in sublist]\n",
    "            print(\"all_ads_fetched: \", len(all_ads_fetched))\n",
    "            all_ads_fetched = all_ads_fetched[:number_to_return+1]\n",
    "            print(\"all_ads_fetched: \", len(all_ads_fetched))\n",
    "            #print(\"all_ads_fetched\", all_ads_fetched)\n",
    "    \n",
    "            '''\n",
    "            while (retrived_items_counter< requested_returned_ads_count):\n",
    "                if main_page_flag==0:\n",
    "    \n",
    "                    # Check if the ChromeDriver is initialized\n",
    "                    if driver is not None:\n",
    "                        print(\"ChromeDriver is initialized and ready to use.\")\n",
    "                        page_scrape_status, counted_items = scrape_page_flow(driver, 1)\n",
    "                        retrived_items_counter = retrived_items_counter+counted_items\n",
    "                        main_page_flag=1\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(\"ChromeDriver failed to initialize.\")\n",
    "                        break\n",
    "                    \n",
    "                    \n",
    "    \n",
    "                else:\n",
    "                    navigate_to_the_next_page_status = navigate_to_the_next_page(driver , 2)\n",
    "                    if navigate_to_the_next_page_status:\n",
    "                        time.sleep(20)\n",
    "                        scrap_second_page_status, counted_items  = scrape_page_flow(driver, 2)\n",
    "                        \n",
    "                        if scrap_second_page_status:\n",
    "                            return True\n",
    "                        else:\n",
    "                            return False\n",
    "                    else:\n",
    "                        return False\n",
    "            '''\n",
    "            status = True\n",
    "    \n",
    "        except Exception as e:\n",
    "            \n",
    "            print(\"An error occurred:\", e)\n",
    "            status = False\n",
    "            all_ads_fetched=[]\n",
    "            \n",
    "        finally:\n",
    "            print(\"print i quit\")\n",
    "            driver.quit()\n",
    "            \n",
    "    \n",
    "        return status, all_ads_fetched\n",
    "\n",
    "\n",
    "\n",
    "def SEND_SAMPLE_TO_EMAIL(sender_email, sender_password, recipient_email, subject, body):\n",
    "    # Create a multipart message\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = recipient_email\n",
    "    msg['Subject'] = subject\n",
    "\n",
    "    # Attach the email body\n",
    "    msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "    try:\n",
    "        # Set up the server\n",
    "        server = smtplib.SMTP('smtp.gmail.com', 587)  # Gmail SMTP server\n",
    "        server.starttls()  # Upgrade the connection to a secure encrypted SSL/TLS connection\n",
    "        \n",
    "        # Log in to your account\n",
    "        server.login(sender_email, sender_password)  # Use your App Password here\n",
    "\n",
    "        # Send the email\n",
    "        server.send_message(msg)\n",
    "        print(\"Email sent successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email: {e}\")\n",
    "\n",
    "    finally:\n",
    "        server.quit()  # Close the connection\n",
    "\n",
    "\n",
    "def keyword_preprocessing(search_keyword):\n",
    "    if search_keyword is None or search_keyword == \"null\" or (isinstance(search_keyword, float) and np.isnan(search_keyword)):\n",
    "        search_keyword=\"\"\n",
    "\n",
    "    modified=False\n",
    "    \n",
    "    # Remove special characters \n",
    "    cleaned_search_keyword = re.sub(r'[^A-Za-zء-ي0-9\\s]', '', search_keyword) # Remove extra spaces \n",
    "    cleaned_search_keyword = re.sub(r'\\s+', ' ', cleaned_search_keyword).strip()\n",
    "    if (cleaned_search_keyword!=search_keyword):\n",
    "        modified=True\n",
    "    \n",
    "    #if (cleaned_search_keyword!=\"\") | (cleaned_search_keyword!=None) | (cleaned_search_keyword!=np.nan):\n",
    "    # Check if the variable is not an empty string, None, or NaN\n",
    "    if (cleaned_search_keyword != \"\"):\n",
    "        status = True\n",
    "    else:\n",
    "        status = False\n",
    "\n",
    "    return status, modified, cleaned_search_keyword\n",
    "\n",
    "\n",
    "def check_email_is_valid(email):\n",
    "    modified=False\n",
    "    # Define the regular expression for a valid email address\n",
    "    email_regex = r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$'\n",
    "    \n",
    "    # Check if the email matches the regular expression\n",
    "    status = re.match(email_regex, email) is not None\n",
    "    return status, modified, email\n",
    "\n",
    "\n",
    "def check_sample_size_valid(sample_size):\n",
    "    modified=False\n",
    "    if sample_size < 20:\n",
    "        status =False\n",
    "        updated_sample_size = 20\n",
    "    else:\n",
    "        status =True\n",
    "        updated_sample_size = sample_size\n",
    "\n",
    "    if (updated_sample_size!=sample_size):\n",
    "        modified=True\n",
    "\n",
    "    return status, modified, updated_sample_size\n",
    "\n",
    "def PROCESS_REQUEST(request_components):\n",
    "\n",
    "    try:\n",
    "        preprocessing_status, search_keyword_modification_flag     , search_keyword = keyword_preprocessing(request_components['search_keyword'])\n",
    "        \n",
    "        reciver_email_status, reciver_email_modification_flag      , reciver_email          = check_email_is_valid(request_components['reciver_email'])\n",
    "        \n",
    "        sample_size_status  , sample_size_modification_flag        , updated_sample_size    = check_sample_size_valid(request_components['size'])\n",
    "\n",
    "        \n",
    "        # Check for valid input\n",
    "        if ((not preprocessing_status) or (not reciver_email_status) or (not sample_size_status)):\n",
    "            status = False\n",
    "\n",
    "        else:\n",
    "            status = True\n",
    "\n",
    "    except:\n",
    "        status = False\n",
    "\n",
    "\n",
    "    return status, search_keyword, reciver_email, updated_sample_size\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def CHECK_KEYWORD_IN_DB_TODAY(search_keyword, sample_size):\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    query = {\"keyword\": search_keyword, \"date\": today}\n",
    "\n",
    "    #TODO\n",
    "    # Check if data exists for today\n",
    "    #existing_data = mongo.db.ads.find_one(search_keyword)\n",
    "\n",
    "    existing_data = {\"results\": [\n",
    "    {\"id\": 1, \"name\": \"Product A\", \"price\": 10.99},\n",
    "    {\"id\": 2, \"name\": \"Product B\", \"price\": 5.99},\n",
    "    {\"id\": 3, \"name\": \"Product C\", \"price\": 15.99},\n",
    "    {\"id\": 4, \"name\": \"Product D\", \"price\": 8.49},\n",
    "    {\"id\": 5, \"name\": \"Product E\", \"price\": 12.99},\n",
    "    {\"id\": 6, \"name\": \"Product F\", \"price\": 7.49},\n",
    "    {\"id\": 7, \"name\": \"Product G\", \"price\": 4.99},\n",
    "    {\"id\": 8, \"name\": \"Product H\", \"price\": 9.99},\n",
    "    {\"id\": 9, \"name\": \"Product I\", \"price\": 6.99},\n",
    "    {\"id\": 10, \"name\": \"Product J\", \"price\": 14.99},\n",
    "    {\"id\": 11, \"name\": \"Product K\", \"price\": 11.49},\n",
    "    {\"id\": 12, \"name\": \"Product L\", \"price\": 3.99},\n",
    "    {\"id\": 13, \"name\": \"Product M\", \"price\": 16.49},\n",
    "    {\"id\": 14, \"name\": \"Product N\", \"price\": 2.99},\n",
    "    {\"id\": 15, \"name\": \"Product O\", \"price\": 13.49},\n",
    "    {\"id\": 16, \"name\": \"Product P\", \"price\": 8.99},\n",
    "    {\"id\": 17, \"name\": \"Product Q\", \"price\": 5.49},\n",
    "    {\"id\": 18, \"name\": \"Product R\", \"price\": 1.99},\n",
    "    {\"id\": 19, \"name\": \"Product S\", \"price\": 12.49},\n",
    "    {\"id\": 20, \"name\": \"Product T\", \"price\": 11.99}\n",
    "    ]\n",
    "                    }\n",
    "\n",
    "    if (sample_size>len(existing_data['results'])):\n",
    "        existing_data=False\n",
    "\n",
    "    return existing_data\n",
    "'''\n",
    "\n",
    "def CHECK_KEYWORD_IN_DB_TODAY(search_keyword, sample_size):\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    query = {\"keyword\": search_keyword, \"date\": today}\n",
    "    \n",
    "    existing_data = retrieve_from_mongo('olx_ads', 'ads_temp', query)\n",
    "\n",
    "    if (sample_size>len(existing_data)):\n",
    "        existing_data=False\n",
    "\n",
    "    else:\n",
    "        existing_data = existing_data[:sample_size]\n",
    "\n",
    "    return existing_data\n",
    "\n",
    "def SCRAPE_ADS(search_keyword):\n",
    "    #ads_dict=\n",
    "    scrape_ads_status=False\n",
    "    ads_dict={}\n",
    "    return scrape_ads_status, ads_dict\n",
    "\n",
    "'''\n",
    "def SCRAPER(search_keyword):\n",
    "    scrape_process_status= False\n",
    "\n",
    "    #2.1- scrape ads \n",
    "    scrape_ads_status, ads_dict = SCRAPE_ADS(search_keyword)\n",
    "    if scrape_ads_status:\n",
    "        print(\"2.1- SCRAPE_ADS Status Success...\")\n",
    "        \n",
    "        #2.2- save output of scraper to mongodb\n",
    "        save_to_db_status = SAVE_RESULT_TO_DB(ads_dict)\n",
    "        if save_to_db_status:\n",
    "            print(\"2.2- SAVE_RESULT_TO_DB Status Sucess\")\n",
    "            scrape_process_status= {\"status\":\"success\", \"status_description\": \"Save Scraped data to DB process succeded.\"}\n",
    "\n",
    "        else:\n",
    "            print(\"2.2- SAVE_RESULT_TO_DB Status  Failed...\")\n",
    "            scrape_process_status= {\"status\":\"error\", \"status_description\": \"Save Scraped data to DB falied due to 2- scrape_ads_status.\"}\n",
    "\n",
    "    else:\n",
    "        print(\"2.1- SCRAPE_ADS Status Failed...\")\n",
    "        scrape_process_status= {\"status\":\"error\", \"status_description\": \"Scrape process falied due to 2- scrape_ads_status.\"}\n",
    "\n",
    "    return Scrape_process_status\n",
    "'''       \n",
    "        \n",
    "'''\n",
    "def RETRIVE_DATA(search_keyword):\n",
    "\n",
    "    check_result = CHECK_KEYWORD_IN_DB_TODAY(search_keyword)\n",
    "    \n",
    "    if check_result:\n",
    "        ads = existing_data['results']\n",
    "    else:\n",
    "        # Scrape data\n",
    "        ads = SCRAPER(keyword, max(size, 300))\n",
    "\n",
    "    return  scrap_result\n",
    "'''\n",
    "\n",
    "def list_to_string(input_list):\n",
    "    result = []\n",
    "\n",
    "    for item in input_list:\n",
    "        if isinstance(item, dict):\n",
    "            # Convert dictionary to string\n",
    "            dict_string = ', '.join(f\"{key}: {value}\" for key, value in item.items())\n",
    "            result.append(f\"{{ {dict_string} }}\")  # Add curly braces for dictionary representation\n",
    "        elif isinstance(item, str):\n",
    "            result.append(item)\n",
    "        else:\n",
    "            result.append(str(item))  # Convert other types to string if necessary\n",
    "\n",
    "    return ' '.join(result)  # Join all elements into a single string\n",
    "\n",
    "\n",
    "def RETRIVE_DATA(search_keyword, sample_size):\n",
    "    check_result = CHECK_KEYWORD_IN_DB_TODAY(search_keyword, sample_size)\n",
    "\n",
    "    if check_result:\n",
    "        ads = check_result\n",
    "        status = True\n",
    "    else:\n",
    "        # Scrape data\n",
    "        status, ads = SCRAPER(search_keyword, max(sample_size, 300))\n",
    "    \n",
    "    ads = ads[:max(sample_size, 300)]\n",
    "    #ads = \" \".join(ads)\n",
    "    ads= list_to_string(ads)\n",
    "    return status,  ads\n",
    "\n",
    "\n",
    "def full_process(request_data):\n",
    "    full_process_status= False\n",
    "\n",
    "    #1- processing the request\n",
    "    request_processing_status, search_keyword, reciver_email, sample_size = PROCESS_REQUEST(request_data)\n",
    "    if request_processing_status:\n",
    "        print(\"1-PROCESS_REQUEST Status Success...\")\n",
    "    \n",
    "        #2- retrive data whether from the database  or scraper \n",
    "        retriving_data_status, retrived_sample_data = RETRIVE_DATA(search_keyword, sample_size)\n",
    "        if retriving_data_status:\n",
    "            print(\"2-RETRIVE_DATA Status Success...\")\n",
    "            full_process_status = {\"status\":\"success\", \"status_description\": \"full process succeded.\"}\n",
    "\n",
    "        else:\n",
    "            print(\"2-RETRIVE_DATA Status Failed...\")\n",
    "            full_process_status= {\"status\":\"error\", \"status_description\": \"full process falied due to 2- scraper_status.\"}\n",
    "    else:\n",
    "        retrived_sample_data=\"\"\n",
    "        print(\"1-PROCESS_REQUEST Status Failed...\")\n",
    "        full_process_status= {\"status\":\"error\", \"status_description\": \"full process falied due to bad inputs, make sure you entered a sample more than 20 , a valid email and a valid keyword.\"}\n",
    "        \n",
    "    return full_process_status, retrived_sample_data\n",
    "\n",
    "\n",
    "@app.route('/retrive_ads', methods=['POST'])\n",
    "def retrive_ads():\n",
    "    data = request.get_json()\n",
    "    print(request)\n",
    "    print(\"request printed\")\n",
    "    status, sample_str = full_process(data)\n",
    "        \n",
    "    sender_email = os.getenv('MAIL_USERNAME')\n",
    "    sender_password = os.getenv('MAIL_PASSWORD')\n",
    "    recipient_email = data['reciver_email']\n",
    "    subject =\"retrive sample of requested ads\"\n",
    "    body = sample_str\n",
    "\n",
    "    #SEND_SAMPLE_TO_EMAIL(sender_email, sender_password, recipient_email, subject, body)\n",
    "    return jsonify(status)\n",
    "\n",
    "\n",
    "\n",
    "print(\">>>>>>>>>>> APIs are up running...\") \n",
    "\n",
    "if  __name__ == '__main__':\n",
    "\n",
    "    print(\">>>>>>>>>>> Application Works..\") \n",
    "    debug = os.getenv(\"DEBUG\")  # Access the DEBUG environment variable\n",
    "    if debug:\n",
    "        app.debug = True\n",
    "    else:\n",
    "        app.debug = False\n",
    "    app.run(port=4996 ,  use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a67a0-3f79-4def-b1ab-aa6e4126eef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb4c83-ef35-4001-829b-af0ceafd73c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
